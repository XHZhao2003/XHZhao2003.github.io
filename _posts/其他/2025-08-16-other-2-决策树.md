---
layout: post
title: "决策树、梯度提升、XGBoost"
category: "其他"
order: "2"
math: true
---

### 决策树

决策树（Decision Tree）是一种树形结构的机器学习模型，它通过一系列 “是/否” 问题来对数据进行分类或预测。我们以 CART（Classification and Regression Tree）框架下的回归树进行形式化定义。

给定一个训练数据集

$$
\mathcal{D} = \{(x_1, y_1), (x_2, y_2), \cdots, (x_n, y_n)\}
$$

其中 $x_i \in \mathbb{R}^d$ 表示第 $ i $ 个样本的特征向量，$y_i \in \mathbb{R}$ 表示该样本的目标值。在回归任务下，$y_i$ 是连续值；在分类任务下，$y_i$ 是离散值，例如 $\{1, -1\}$。

一棵决策树由以下部分构成：
- 内部结点：每个结点对应一个分裂规则：$x^{(j)} \leq s$，其中 $j$ 是特征维度，$s$ 是分裂阈值
- 叶子结点：每个叶子 $l$ 对应一个预测值 $w_l \in \mathbb{R}$。
- 树结构 $T$：定义了一个从输入空间到叶子索引的映射函数 $q: \mathbb{R}^d \to \{1,2,\cdots, L\}$，其中 $L$ 是叶子数量。
- 预测函数即为 $f(x)=w_{q(x)}$。

我们希望找到树结构 $T$ 和叶子权重 $\{w_l\}_{l=1}^L$，来最小化训练集上的误差。对于回归任务，我们使用平方误差：

$$
\min_{T, \{w_i\}} \sum_{i=1}^n(y_i - w_{q(x_i)})^2
$$

由于阈值分裂规则是不连续的，无法直接使用梯度优化，因此采用贪心递归分裂算法来最小化上述损失。下面的伪代码描述了这一算法。

```
# 输入：数据集 D, 最小叶样本数 min_samples_split, 最大数深度 max_depth, 当前深度 depth
# 输出：决策树结点（内部结点或者叶子结点）
function BuildTree(D, min_samples_split=2, max_depth=inf, depth=0)
    # 如果满足停止条件，返回叶子结点
    if |D| < min_samples_split or depth >= max_depth or all_i are equal:
        w = mean(y_i for (x_i, y_i) in D)
        return LeadNode(weight=w)

    # 否则，寻找最优分裂
    best_split = None
    best_loss_reduction = -inf

    # 遍历所有特征 j 和所有可能的分裂点 s
    for j in range(d):  # d 是特征维数
        values = {x_i[j] for (x_i, y_i) in D}
        thresholds = generate_threshold(values)

        for s in thresholds:
            D_left  = {(x_i, y_i) in D | x_i[j] <= s}
            D_right = {(x_i, y_i) in D | x_i[j] >  s}

            # 左右子树必须非空
            if |D_left| == 0 or |D_right| == 0:
                continue

            # 计算分裂前后的损失变化，预测值用 y 的平均值表示
            loss_before = sum((y - mean(y_all)) ** 2 for y in y_all)
            loss_after = sum((y - mean(y_left)) ** 2 for y in y_left) + sum((y - mean(y_right) ** 2) for y in y_right)

            loss_reduction = loss_before - loss_after

            if loss_reduction > best_loss_reduction:
                best_loss_reduction = loss_reduction
                best_split = (j, s, D_left, D_right)

    # 如果没有有效分裂，返回叶子
    if best_split is None:
        w = mean(y_i fpr (x_i, y_i) in D)
        return LeafNode(weight=w)

    (j, s, D_left, D_right) = best_split

    # 递归构建左右子树
    left_subtree = BuildTree(D_left, min_samples_split, max_depth, depth + 1)
    right_subtree = BuildTree(D_right, min_samples_split, max_depth, depth + 1)

    return InternalNode(feature=j, threshold=s, left=left_subtree, right=right_subtree)
```

在推理时，给定输入特征 $x$，只需要从决策树的根节点出发，按照每个结点上的规则导航到叶结点，就得到了模型预测值。

上面的例子中，模型预测任务是回归任务，因此构建树的过程中的 loss 采用的是 $L_2$ 损失。对于分类问题来说，我们还可以采用信息增益、基尼不纯度等指标作为损失函数，来指导树的构建过程。

**信息增益 IG**

信息增益（Information Gain, IG）借用信息论中的信息熵概念，将训练集的信息熵作为损失函数。信息熵表示的是 $y$ 值的不确定程度。对于决策树中的一个结点 $v$，训练集上能够被导航到这个结点的所有样本记为 $D$，其真实标签为 $Y=\{y_i\}_{i=1}^{|D|}$ ，样本集 $D$ 的类别分布记为 
$$
p_c = \frac{|\{(x_i, y_i) | y_i \in c\}}{|D|}|, c=1, \cdots, C
$$
那么其信息熵为
$$
H(Y)=-\sum_{c=1}^Cp_c \log_2 p_c
$$
假设结点 $v$ 基于某个特征 $x_t$ ，将其取值空间划分为 $V$ 个子区间 $X_t^1, \cdots, X_t^V$，从而将样本集 $D$ 划分为 $D_{1},\cdots D_{V}$，此时真实标签 $Y$ 对于 $x_t$ 的条件信息熵是
$$
H(Y|x_t) = \sum_{i=1}^V\frac{|D_i|}{|D|}H(Y|x_t\in X_t^i)
$$
那么 $x_t$ 与 $Y$ 的互信息就是这条规则带来的信息增益
$$
IG(D;x_t, X_t^1, \cdots, X_t^V)=I(x_t, Y)=H(Y)-H(Y|x_t)
$$
ID3 使用的就是 IG 来生成决策树，也就是说，在上述伪代码中遍历各种特征取值划分方案时，用信息增益来作为 loss 指标，作为对最终的分类效果的估计。

**信息增益率**

信息增益率可以校正取值个数带来的影响，其定义为 $IG$ 与 $H(X_t)$ 之比。也就是如果 $X_t$ 本身的信息熵就特别大，那么信息增益率就很小。极端的例子是，如果把样本 id 当作特征，还让每个样本自成一组，那么经过这一次分类之后就没有任何不确定性了，但是形成的决策树也没有意义。这种情况下，它的信息增益率会很小。C4.5决策树的做法是，从候选划分特征中先找出信息增益高于平均水平的属性，再从中选出增益率最高的。

**基尼系数**

样本集 $D$ 的纯度可以用基尼值来衡量，它的含义是，从 $D$ 中随机抽两个样本，其类别标记不一致的概率。
$$
G(D)=\sum_{i=1}^{|C|}\sum_{j\neq i}p_i\times p_j =   1-\sum_{c=1}^Cp_c^2
$$
基尼值 $G(D)$ 越小，表示大概率两个样本的类别相同，样本集的纯度越高。

在结点 $v$ 处，如果应用规则将样本集 $D$ 分为子样本集 $D_1, \cdots, D_V$，则其基尼减少量（不纯度减少量）为
$$
\Delta G = G(D) - \sum_{i=1}^V\frac{|D_i|}{|D|}G(D_i)
$$
CART 分类树就是使用基尼系数作为 loss 指标，每次筛选特征时选择使不纯度减少最多的方案。

### 梯度提升决策树（Gradient Boosting）

Boosting 算法的一般描述如下。给定训练集 $\mathcal{D} = \{(x_1, y_1), (x_2, y_2), \cdots, (x_n, y_n)\}, x_i \in \mathbb{R^d}, y_i \in \mathbb{R}$ ，我们希望学习一个预测函数 $F:\mathbb{R^d}\to \mathbb{R}$，使得某个损失函数 $L(y, F(x))$ 最小化。但是真实的映射过于复杂，简单的机器学习模型可能不足以描述它（深度学习就是用神经网络，把 $F$ 的结构搞得很复杂）。Boosting 算法的解决办法是，用加法模型来逼近真实映射：
$$
F_M(x)=F_0(x)+\sum_{m=1}^M f_m(x)
$$
其中，$F_0(x)$ 是初始猜测，例如是一个常数。$f_m(x)$ 是第 $m$ 轮加入的基学习器，例如一个决策树模型。这样，后来加入的基学习器其实是在预测已有的加法模型的误差。

为了训练这个加法模型，逐步构建各个部分。对于初始猜测，我们用优化问题求解它：
$$
F_0(x) = \arg\min_{\gamma} \sum_{i=1}^n L(y_i, \gamma)
$$
对于 $m=1, 2, \cdots M$，$f_m$ 要优化的损失函数就是
$$
L_m(y_i, x_i)=L(y_i, F_{m-1}(x_i)+f(x_i))
$$

$$
f_m = \arg\min_{f\in \mathcal{H}} \sum_{i=1}^n L_m(y_i, x_i)
$$

其中 $\mathcal{H}$ 是基学习器空间，例如所有深度为 3 的决策树模型。

Gradient Boosting 的想法是，既然后来加入的基学习器就是在预测已有的加法模型的误差，那么我们就把它看作是梯度优化过程。所以调整一下训练目标：对于样本 $(x_i, y_i)$，$f_m(x_i)$ 不尝试拟合误差 $y_i - F_{m-1}(x_i)$，而是尝试拟合 $F_{m-1}(x_i)$ 在损失函数中的负梯度，也就是
$$
r_{im} = -\left. \frac{\partial L(y_i, F(x_i))}{\partial F(x_i)} \right|_{F(x_i)=F_{m-1}(x_i)}
$$
 对于常用的损失函数，这个负梯度是：
$$
L(y, F(x))=\frac{1}{2}(y-F(x))^2,\quad r_{im} = y_i - F_{m-1}(x_i) \\
L(y, F(x))=\log(1+e^{-yF(x)}), \quad r_{im}= \frac{y_i}{1+e^{y_iF_{m-1}(x_i)}}
$$
这种设定下，还会有一个学习率步长 $\alpha$，加法模型的真实更新方式是 $F_m = F_{m-1} + \alpha f_m$。

而且，即使是分类任务，我们也会采用回归树作为基学习器，因为它预测的并不是标签，而是“如何调整当前模型”的梯度。对于分类任务来说，加法模型的理想输出是 $\{1, -1\}$，但实际的输出会是连续实数。
