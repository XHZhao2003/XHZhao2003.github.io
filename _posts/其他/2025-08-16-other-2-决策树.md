---
layout: post
title: "决策树、梯度提升、XGBoost"
category: "其他"
order: "2"
---

决策树（Decision Tree）是一种树形结构的机器学习模型，它通过一系列 “是/否” 问题来对数据进行分类或预测。我们以 CART（Classification and Regression Tree）框架下的回归树进行形式化定义。

给定一个训练数据集

$$
\mathcal{D} = {(x_1, y_1), (x_2, y_2), \cdots, (x_n, y_n)}
$$

其中 $x_i \in \mathbb{R}^d$ 表示第 $ i $ 个样本的特征向量，$y_i \in \mathbb{R}$ 表示该样本的目标值。在回归任务下，$y_i$ 是连续值；在分类任务下，$y_i$ 是离散值，例如 $\{1, -1\}$。

一棵决策树由以下部分构成：
- 内部结点：每个结点对应一个分裂规则：$x^{(j)} \leq s$，其中 $j$ 是特征维度，$s$ 是分裂阈值
- 叶子结点：每个叶子 $l$ 队以ing一个预测值 $w_l \in \mathbb{R}$。
- 树结构 $T$：定义了一个从输入空间到叶子索引的映射函数 $q: \mathbb{R}^d \to \{1,2,\cdots, L\}$，其中 $L$ 是叶子数量。
- 预测函数即为 $f(x)=w_{q(x)}$。

我们希望找到树结构 $T$ 和叶子权重 $\{w_l\}_{l=1}^L$，来最小化训练集上的误差。对于回归任务，我们使用平方误差：

$$
\min_{T, \{w_i\}} \sum_{i=1}^n(y_i - w_{q(x_i)})^2
$$

由于阈值分裂规则是不连续的，无法直接使用梯度优化，因此采用贪心递归分裂算法来最小化上述损失。下面的伪代码描述了这一算法。

```
# 输入：数据集 D, 最小叶样本数 min_samples_split, 最大数深度 max_depth, 当前深度 depth
# 输出：决策树结点（内部结点或者叶子结点）
function BuildTree(D, min_samples_split=2, max_depth=int, depth=0)
    # 如果满足停止条件，返回叶子结点
    if |D| < min_samples_split or depth >= max_depth or all_i are equal:
        w = mean(y_i for (x_i, y_i) in D)
        return LeadNode(weight=w)

    # 否则，寻找最优分裂
    best_split = None
    best_loss_reduction = -inf

    # 遍历所有特征 j 和所有可能的分裂点 s
    for j in range(d):  # d 是特征维数
        values = {x_i[j] for (x_i, y_i) in D}
        thresholds = generate_threshold(values)

        for s in thresholds:
            D_left  = {(x_i, y_i) in D | x_i[j] <= s}
            D_right = {(x_i, y_i) in D | x_i[j] >  s}

            # 左右子树必须非空
            if |D_left| == 0 or |D_right| == 0:
                continue

            # 计算分裂前后的损失变化，预测值用 y 的平均值表示
            loss_before = sum((y - mean(y_all)) ** 2 for y in y_all)
            loss_after = sum((y - mean(y_left)) ** 2 for y in y_left) + sum((y - mean(y_right) ** 2) for y in y_right)

            loss_reduction = loss_before - loss_after

            if loss_reduction > best_loss_reduction:
                best_loss_reduction = loss_reduction
                best_split = (j, s, D_left, D_right)

    # 如果没有有效分裂，返回叶子
    if best_split is None:
        w = mean(y_i fpr (x_i, y_i) in D)
        return LeafNode(weight=w)

    (j, s, D_left, D_right) = best_split

    # 递归构建左右子树
    left_subtree = BuildTree(D_left, min_samples_split, max_depth, depth + 1)
    right_subtree = BuildTree(D_right, min_samples_split, max_depth, depth + 1)

    return InternalNode(feature=j, threshold=s, left=left_subtree, right=right_subtree)


```


